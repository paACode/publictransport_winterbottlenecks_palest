---
title: "dataset_requirements"
output: html_document
---

As a first step the data from November 2024 were downloaded from the [opentransport archive](https://archive.opentransportdata.swiss/actual_data_archive.html).


```{r Download Data}
url <-"https://archive.opentransportdata.swiss/istdaten/2024/ist-daten-2024-11.zip"
destination_folder <- file.path(getwd(), "raw_data")
destination_zip <- file.path(destination_folder, "ist-daten-2024-11.zip")
unzipped_folder <- file.path(destination_folder, "ist-daten-2024-11")


is_already_downloaded <- function() {
  csv_files <- list.files(file.path(destination_folder, "ist-daten-2024-11"), pattern = "\\.csv$")
  if(length(csv_files)== 30){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}
  
if (is_already_downloaded()){
  print("Raw Data already downloaded")
}else{
  print("Download and unzip takes a few minutes")
  
  
  dir.create(destination_folder, showWarnings = FALSE, recursive = TRUE) 
  download.file(url, destination_zip , mode = "wb")
  unzip(destination_zip, exdir = unzipped_folder)
}


```
The data focused on in this project pertains to trains from Zentralbahn in November, with appropriate filters selected to yield a resulting dataset of 91'271 entries, a moderate size (< 10^5), making it an ideal choice for analysis.

```{r Filter and merge Data}
library(dplyr)

if (file.exists("zentrahlbahn_nov_dataset.csv")){
  print("Zentrahlbahn dataset has already been created")
}else{
  print("Creation of Zentrahlbahn dataset might take a few minutes")
  csv_files <- list.files(file.path(destination_folder, "ist-daten-2024-11"), pattern = "\\.csv$")
  
  merged_data <-  data_frame()
  for(csv_file in csv_files){
    file_name <- file.path(unzipped_folder, csv_file)
    data <-  read.csv(file = file_name, sep =";", head=TRUE)
    
    tmp_filtered <- data %>% 
      filter(PRODUKT_ID == "Zug") %>% 
      filter(BETREIBER_ABK == "ZB") %>% 
      filter(HALTESTELLEN_NAME != "" & !is.na(HALTESTELLEN_NAME)) 
    
    merged_data <-  rbind(merged_data, tmp_filtered)
  } 
  
  write.csv(merged_data, "zentrahlbahn_nov_dataset_raw.csv", row.names = FALSE)
  
}

```

```{r Inspect Raw Data}
raw_data <-  read.csv(file = "zentrahlbahn_nov_dataset_raw.csv", sep =",", head=TRUE )
str(raw_data)
```
To simplify calculations, the data types were manually set, as the raw dataset treats all data as characters.

```{r Define Datatypes}
library(dplyr)
data.dtypes <- raw_data %>%
  mutate(
    BETRIEBSTAG= as.Date(BETRIEBSTAG, format = "%d.%m.%Y"),
    FAHRT_BEZEICHNER = FAHRT_BEZEICHNER, # ???
    BETREIBER_ID = BETREIBER_ID, # ???
    BETREIBER_ABK = as.factor(BETREIBER_ABK), 
    BETREIBER_NAME = as.factor(BETREIBER_NAME), # ZVV, ZVA, etc
    PRODUKT_ID = as.factor(PRODUKT_ID),  # Zug , Bus etc
    LINIEN_ID = LINIEN_ID, # ???
    LINIEN_TEXT = as.factor(LINIEN_TEXT), # S26, IR16 etc
    UMLAUF_ID = UMLAUF_ID, # ???
    VERKEHRSMITTEL_TEXT = as.factor(VERKEHRSMITTEL_TEXT),
    ZUSATZFAHRT_TF = as.logical(ZUSATZFAHRT_TF), 
    FAELLT_AUS_TF = as.logical(FAELLT_AUS_TF),
    BPUIC = BPUIC, # ???
    HALTESTELLEN_NAME = as.factor(HALTESTELLEN_NAME),
    ANKUNFTSZEIT = as.POSIXct(ANKUNFTSZEIT, format = "%d.%m.%Y %H:%M" ),
    AN_PROGNOSE = as.POSIXct(AN_PROGNOSE, format = "%d.%m.%Y %H:%M:%S"),
    AN_PROGNOSE_STATUS = as.factor(AN_PROGNOSE_STATUS),
    ABFAHRTSZEIT = as.POSIXct(ABFAHRTSZEIT, format = "%d.%m.%Y %H:%M" ),
    AB_PROGNOSE = as.POSIXct(AN_PROGNOSE,  format = "%d.%m.%Y %H:%M:%S"),
    AB_PROGNOSE_STATUS = as.factor(AB_PROGNOSE_STATUS), 
    DURCHFAHRT_TF = as.logical(DURCHFAHRT_TF)
    
  )

str(data.dtypes)
```

```{r  Remove Factors with only one level}
# See current state
factor_levels <- sapply(data.dtypes, function(x) if(is.factor(x)) levels(x) else NULL)
# Print the factor levels
factor_levels

#Remove Factors with only one level --> "ZB, "ZENTRAHLBAHN" , "ZUG"
data.dtypes <- data.dtypes[, sapply(data.dtypes, function(x) !(is.factor(x) && length(levels(x)) == 1))]

#Double Check
factor_levels <- sapply(data.dtypes, function(x) if(is.factor(x)) levels(x) else NULL)
# Print the factor levels
factor_levels

```
In oder to improve readablity, it was decided to add delays in minutes, beside in seconds. Further, the regarding columns are enhanved by units which do not effect the value type while supporting readability.
```{r  Add Delay and create dataset}
data.delay <- data.dtypes %>% 
  mutate(ABFAHRTDELAY_sec = difftime(AB_PROGNOSE, ABFAHRTSZEIT, units = "secs"),
         ABFAHRTDELAY_min = difftime(AB_PROGNOSE, ABFAHRTSZEIT, units = "mins"),
         ANKUNFTDELAY_sec = difftime(AN_PROGNOSE, ANKUNFTSZEIT, units = "secs"),
         ANKUNFTDELAY_min = difftime(AN_PROGNOSE, ANKUNFTSZEIT, units = "mins"))

#write.csv(data.delay, "zentrahlbahn_nov_dataset_delay.csv", row.names = FALSE)
head(data.delay)
```

In an additional step, trains are aggregated by Fart_Bezeichner variable and therefore departure and arrival destination locations are extracted and added.
```{r Add Train Start / Stop Locations}

data.processed <- data.delay %>% 
  # Group by train ID (FAHRT_BEZEICHNER)
  group_by(FAHRT_BEZEICHNER) %>%
  mutate(
    # For START, take the HALTESTELLEN_NAME where both ANKUNFTSZEIT and AN_PROGNOSE are missing
    START = first(as.character(HALTESTELLEN_NAME)[is.na(ANKUNFTSZEIT) & is.na(AN_PROGNOSE)]),
    # For ZIEL, take the HALTESTELLEN_NAME where both ABFAHRTSZEIT and AB_PROGNOSE are missing
    ZIEL  = first(as.character(HALTESTELLEN_NAME)[is.na(ABFAHRTSZEIT)])
  ) %>%
  ungroup()
  

```


Also delay categories are created, clustering delays in on time, minor, moderate and significant delays.
```{r delay categories}
# Define the delay categories
data.processed <- data.processed %>%
  mutate(Delay_Category = case_when(
    ANKUNFTDELAY_min < 3 ~ "On Time",
    ANKUNFTDELAY_min >= 3 & ANKUNFTDELAY_min < 6 ~ "Minor Delay",
    ANKUNFTDELAY_min >= 6 & ANKUNFTDELAY_min < 11 ~ "Moderate Delay",
    ANKUNFTDELAY_min >= 11 ~ "Significant Delay",
    ANKUNFTDELAY_min < 0 ~ "Early Arrival",  # Optional: For early arrivals
    TRUE ~ "Unknown"  # Fallback for unexpected values
  ))
```


```{r add day times}
str(data.processed)

data.processed <- data.processed %>%
  mutate(
    TAGESZEIT = { ## Calculate Tageszeiten 
      stunde <- as.numeric(format(as.POSIXct(ANKUNFTSZEIT, format = "%Y-%m-%d %H:%M:%S"), "%H"))
      case_when(
        stunde >= 6  & stunde <= 10 ~ "Vormittag",
        stunde >= 11 & stunde <= 13 ~ "Mittag",
        stunde >= 14 & stunde <= 16 ~ "Nachmittag",
        stunde >= 17 & stunde <= 19 ~ "Abend",
        (stunde >= 20 & stunde <= 23) | (stunde >= 0 & stunde <= 5) ~ "Nacht",
        TRUE ~ NA_character_
      )
    },
    RUSH_HOUR = {
      stunde <- as.numeric(format(as.POSIXct(ANKUNFTSZEIT, format = "%Y-%m-%d %H:%M:%S"), "%H"))
      case_when(
        is.na(stunde) ~ NA_character_,  # If no valid hour, return NA
        stunde >= 6 & stunde <= 8  ~ "rush_hour_vormittag",
        stunde >= 17 & stunde <= 18 ~ "rush_hour_abend",
        TRUE ~ "rush_hour_none"
      )
    }
  )

```

Import and modify weather data
```{r weather data}
library(dplyr)
library(readr)
library(lubridate)
library(tidyr)

# --- Define metadata and file paths for weather stations ---
stations <- list(
  ENG = list(
    name = "Engelberg", elevation = 1036,
    lat = 46.821639, lon = 8.410514,
    region = "Central Alpine north slope", canton = "OW",
    file = "d.weather/Engelberg_weather_data.csv"
  ),
  LUZ = list(
    name = "Luzern", elevation = 454,
    lat = 47.036439, lon = 8.301022,
    region = "Central plateau", canton = "LU",
    file = "d.weather/Luzern_weather_data.csv"
  ),
  MER = list(
    name = "Meiringen", elevation = 589,
    lat = 46.732222, lon = 8.169247,
    region = "Western Alpine north slope", canton = "BE",
    file = "d.weather/Meiringen_weather_data.csv"
  )
)

# --- Function to load, filter, and annotate weather data for each station ---
process_station <- function(station) {
  df <- read_delim(station$file, delim = ";", show_col_types = FALSE)
  
  # Check that a "date" column exists
  if (!"date" %in% names(df)) {
    stop(paste("No 'date' column found in file:", station$file))
  }
  
  df <- df %>%
    mutate(date = ymd(date)) %>%  # Convert the date column to Date type
    filter(date >= as.Date("2024-11-01") & date <= as.Date("2024-11-30")) %>%  # Filter for November 2024
    mutate(
      station_name = station$name  # add station name for later pivoting
      # (Other metadata are added below manually after pivoting)
    )
  
  return(df)
}

# --- Process all stations and combine into one dataframe ---
weather_all <- bind_rows(lapply(stations, process_station))

# --- Rename weather measurement variables ---
# Adjust these names if your CSV files have different column names.
all_data <- weather_all %>%
  rename(
    # station_code is renamed only if the CSV provides such a column.
    station_code        = `station/location`,   
    date                = date,
    solar_radiation_wm2 = gre000d0,
    snow_depth_cm       = hto000d0,
    cloud_cover_pct     = nto000d0,
    pressure_hpa        = prestad0,
    precip_mm           = rre150d0,
    sunshine_min        = sre000d0,
    temp_avg_c          = tre200d0,
    temp_min_c          = tre200dn,
    temp_max_c          = tre200dx,
    humidity_pct        = ure200d0
  )

# --- Pivot the weather data to wide format with suffix '_w' ---
# This creates, for each date, separate columns per station for each weather variable.
weather_wide <- all_data %>%
  pivot_wider(
    id_cols = date,
    names_from = station_name,
    values_from = c(solar_radiation_wm2, snow_depth_cm, cloud_cover_pct, pressure_hpa, 
                    precip_mm, sunshine_min, temp_avg_c, temp_min_c, temp_max_c, humidity_pct),
    names_glue = "w_{.value}_{station_name}"
  )

# --- Add station metadata for each station ---
# Since the metadata is constant per station, add them as new columns to the wide weather data.
weather_wide <- weather_wide %>%
  mutate(
    # For Engelberg
    w_elevation_Engelberg = stations$ENG$elevation,
    w_lat_Engelberg       = stations$ENG$lat,
    w_lon_Engelberg       = stations$ENG$lon,
    w_region_Engelberg    = stations$ENG$region,
    w_canton_Engelberg    = stations$ENG$canton,
    # For Luzern
    w_elevation_Luzern    = stations$LUZ$elevation,
    w_lat_Luzern          = stations$LUZ$lat,
    w_lon_Luzern          = stations$LUZ$lon,
    w_region_Luzern       = stations$LUZ$region,
    w_canton_Luzern       = stations$LUZ$canton,
    # For Meiringen
    w_elevation_Meiringen = stations$MER$elevation,
    w_lat_Meiringen       = stations$MER$lat,
    w_lon_Meiringen       = stations$MER$lon,
    w_region_Meiringen    = stations$MER$region,
    w_canton_Meiringen    = stations$MER$canton
  )

# --- Merge the weather data with the main dataset ---
# data.processed is assumed to be your train stops tibble with a date column called BETRIEBSTAG.
data.main <- data.processed %>%
  left_join(weather_wide, by = c("BETRIEBSTAG" = "date"))

# --- View the merged data ---
print(data.main)



```

```{r write final file}
  write.csv(data.main, "zentrahlbahn_final.csv", row.names = FALSE)
```